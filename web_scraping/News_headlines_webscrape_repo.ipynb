{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Headlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def perform_initial_setup(driver):\n",
    "    try:\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, '.heading-font-family.fs-20px.text-nowrap'))\n",
    "        )\n",
    "\n",
    "        # cookie\n",
    "        cookie = driver.find_element(\n",
    "            By.XPATH, '/html/body/div[1]/div/div/div[7]/div/div/div[2]/button')\n",
    "        cookie.click()\n",
    "        print(\"cookie clicked\")\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # advanced search\n",
    "        adv_search = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.heading-font-family.fs-20px.text-nowrap')\n",
    "        adv_search.click()\n",
    "        print(\"Advanced search clicked\")\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # period input\n",
    "        period = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.col-auto.my-auto')\n",
    "        period.click()\n",
    "        print(\"Period input field clicked\")\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        year1 = driver.find_element(\n",
    "            By.XPATH, '//button[contains(text(), \"1Y\")]')\n",
    "        year1.click()\n",
    "        print(\"1y option selected\")\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # confirm button\n",
    "        confirm = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.btn.w-50.w-xl-auto.bg-primary.text-white.fs-20px.ms-1.text-nowrap')\n",
    "        confirm.click()\n",
    "        print(\"Confirm button clicked\")\n",
    "        time.sleep(random.uniform(6, 10))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during initial setup: {e}\")\n",
    "\n",
    "\n",
    "def search_stock(driver, stock_name):\n",
    "    try:\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "        # search keyword\n",
    "        search = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.form-control.shadow-none.border-0.px-0.fs-20px')\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollTop);\")\n",
    "        search.clear()\n",
    "        time.sleep(random.uniform(2, 3))\n",
    "        search.send_keys(stock_name)\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        print(f\"{stock_name} searched\")\n",
    "        # load\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located(\n",
    "                (By.XPATH, '//div[@class=\"d-flex me-2\"]//span'))\n",
    "        )\n",
    "\n",
    "        # scroll down to load more \n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(10, 15))\n",
    "\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/div[4]/div[1]/div/div[2]/div/div[1]'))\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            extend_rows = driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/div[4]/div[1]/div/div[2]/div/div[1]')\n",
    "            extend_rows.click()\n",
    "            print(\"Extended rows using the first XPath\")\n",
    "        except Exception as e:\n",
    "            print(f\"First XPath failed: {e}\")\n",
    "            extend_rows = driver.find_element(By.XPATH, '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/div[4]/div[1]/div/div[2]/div/div[1]')\n",
    "            extend_rows.click()\n",
    "            print(\"Extended rows using the second XPath\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 100 rows selecting\n",
    "        select_100 = driver.find_element(\n",
    "            By.XPATH, '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/div[4]/div[1]/div/div[2]/div/div[3]/ul/li[4]/span/div')\n",
    "        select_100.click()\n",
    "        print(\"selected 100\")\n",
    "        time.sleep(random.uniform(7, 10))\n",
    "\n",
    "        loading_complete_xpath = '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/div[4]/div[1]/div/div[2]/div/div[1]'\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, loading_complete_xpath)))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"serach error: {e}\")\n",
    "\n",
    "\n",
    "def scrape_current_page(driver,stock_name):\n",
    "    try:\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollTop);\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "        # Scroll to 1/5 of the page height\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 1 / 5)\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "        # Scroll to 2/5 of the page height\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 2 / 5)\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "        # Scroll to 3/5 of the page height\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 3 / 5)\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "        # Scroll to 4/5 of the page height\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 4 / 5)\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "        # Scroll to 5/5 of the page height \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 5 / 5)\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "        \n",
    "        # Scroll down to load more content \n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(5, 7))\n",
    "\n",
    "        # Find the titles\n",
    "        titles = driver.find_elements(\n",
    "            By.XPATH, '//div[@class=\"d-flex me-2\"]//span')\n",
    "\n",
    "        try:\n",
    "            # date elements\n",
    "            dates = WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.XPATH,\n",
    "                     '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/div[3]/div/div/div/div[1]/div[2]')\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"First XPath failed: {e}\")\n",
    "            dates = WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.XPATH,\n",
    "                     '/html/body/div/div/div/div[2]/div/div[2]/div/div[1]/div/div[2]/div[3]/div[1]/div/div/div[1]/div[2]')\n",
    "                )\n",
    "            )\n",
    "\n",
    "        class_elements = driver.find_elements(\n",
    "            By.XPATH, '//div[@class=\"pb-2 fs-12px text-ref default-font-family fw-500 ps-1\"]')\n",
    "\n",
    "        title_data = [title.text for title in titles]\n",
    "        date_data = [date.text.strip() for date in dates if date.text.strip()]\n",
    "        class_data = [element.text for element in class_elements]\n",
    "        stockname_data = [stock_name for title in titles]\n",
    "        # Print the results\n",
    "        print(\"Titles:\", len(title_data))\n",
    "        print(\"Dates:\", len(date_data))\n",
    "        print(\"Class Elements:\", len(class_data))\n",
    "\n",
    "        data = {\n",
    "            'Title': title_data,\n",
    "            'Date': date_data,\n",
    "            'Class Element': class_data,\n",
    "            'Stock': stockname_data\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def navigate_to_next_page(driver):\n",
    "    try:\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(4, 7))\n",
    "        next_button_xpath = '//button[@aria-label=\"Go to next page\"]'\n",
    "\n",
    "        next_button = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, next_button_xpath))\n",
    "        )\n",
    "        next_button.click()\n",
    "        print(\"Next page button clicked\")\n",
    "        time.sleep(random.uniform(8, 12))\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to navigate to next page: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def scraping(driver, stock_name):\n",
    "    try:\n",
    "        # initial setup (advanced search, period, enter search keyword)\n",
    "        search_stock(driver, stock_name)\n",
    "        all_data = []\n",
    "\n",
    "        # scrape data from the first page\n",
    "        current_df = scrape_current_page(driver, stock_name)\n",
    "        if current_df is not None:\n",
    "            all_data.append(current_df)\n",
    "\n",
    "        # navigate to next pages\n",
    "        page_count = 1\n",
    "        while navigate_to_next_page(driver):\n",
    "            page_count += 1\n",
    "            print(f\"Scraping data from page {page_count}...\")\n",
    "            current_df = scrape_current_page(driver, stock_name)\n",
    "            if current_df is not None:\n",
    "                all_data.append(current_df)\n",
    "\n",
    "            # for testing\n",
    "            # if page_count == 2:\n",
    "            #     break\n",
    "\n",
    "        if all_data:\n",
    "            # f'{stock_name}_df' = pd.concat(all_data, ignore_index=True)\n",
    "            final_df = pd.concat(all_data, ignore_index=True)\n",
    "            today = datetime.today().strftime(\"%d %b %Y\")\n",
    "            final_df['Date'] = final_df['Date'].str.replace('วันนี้', today)\n",
    "            return final_df\n",
    "            # file_name = f\"{stock_name}_data.csv\"\n",
    "            # final_df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "            # print(f\"Data saved to {file_name}\")\n",
    "            # print(final_df)\n",
    "\n",
    "        else:\n",
    "            print(\"No data scraped.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FORTH', 'STGT', 'BSRC', 'THG', 'AEONTS', 'TCAP', 'RCL', 'AURA', 'TASCO', 'RBF', 'MOSHI', 'PLANB', 'TKN', 'TOA', 'CHG', 'DOHOME', 'VGI', 'SJWD', 'SPALI', 'SAPPE']\n",
      "['SISB', 'RATCH', 'BTG', 'TLI', 'GUNKUL', 'BCPG', 'HMPRO', 'IRPC', 'CENTEL', 'MEGA', 'SNNP', 'BCH', 'ICHI', 'EGCO', 'AMATA', 'AAV', 'JMART', 'SPRC', 'AWC', 'INTUCH']\n",
      "['SAWAD', 'COM7', 'BANPU', 'CBG', 'JMT', 'IVL', 'KCE', 'KTC', 'BYD', 'TISCO', 'PTTGC', 'SCGP', 'SCC', 'TIDLOR', 'LH', 'CPN', 'CPF', 'SIRI', 'CRC', 'MTC']\n",
      "['ERW', 'MINT', 'BEM', 'BTS', 'KKP', 'BLA', 'HANA', 'GPSC', 'BH', 'OSP', 'BAM', 'BDMS', 'BGRIM', 'KTB', 'TRUE', 'DELTA', 'PTTEP', 'WHA', 'TTB', 'CPALL']\n",
      "['BCP', 'BBL', 'ADVANC', 'GULF', 'ORI', 'KBANK', 'AOT', 'ITC', 'NEX', 'GLOBAL', 'PTT', 'SCB', 'TOP', 'STA', 'TU', 'CK', 'OR', 'AP', 'M', 'EA']\n"
     ]
    }
   ],
   "source": [
    "stocks = [\n",
    "    \"FORTH\", \"STGT\", \"BSRC\", \"THG\", \"AEONTS\", \"TCAP\", \"RCL\", \"AURA\", \"TASCO\", \"RBF\",\n",
    "    \"MOSHI\", \"PLANB\", \"TKN\", \"TOA\", \"CHG\", \"DOHOME\", \"VGI\", \"SJWD\", \"SPALI\", \"SAPPE\",\n",
    "    \"SISB\", \"RATCH\", \"BTG\", \"TLI\", \"GUNKUL\", \"BCPG\", \"HMPRO\", \"IRPC\", \"CENTEL\", \"MEGA\",\n",
    "    \"SNNP\", \"BCH\", \"ICHI\", \"EGCO\", \"AMATA\", \"AAV\", \"JMART\", \"SPRC\", \"AWC\", \"INTUCH\",\n",
    "    \"SAWAD\", \"COM7\", \"BANPU\", \"CBG\", \"JMT\", \"IVL\", \"KCE\", \"KTC\", \"BYD\",\n",
    "    \"TISCO\", \"PTTGC\", \"SCGP\", \"SCC\", \"TIDLOR\", \"LH\", \"CPN\", \"CPF\", \"SIRI\", \"CRC\",\n",
    "    \"MTC\", \"ERW\", \"MINT\", \"BEM\", \"BTS\", \"KKP\", \"BLA\", \"HANA\", \"GPSC\", \"BH\",\n",
    "    \"OSP\", \"BAM\", \"BDMS\", \"BGRIM\", \"KTB\", \"TRUE\", \"DELTA\", \"PTTEP\", \"WHA\", \"TTB\",\n",
    "    \"CPALL\", \"BCP\", \"BBL\", \"ADVANC\", \"GULF\", \"ORI\", \"KBANK\", \"AOT\", \"ITC\", \"NEX\",\n",
    "    \"GLOBAL\", \"PTT\", \"SCB\", \"TOP\", \"STA\", \"TU\", \"CK\", \"OR\", \"AP\", \"M\",\n",
    "    \"EA\"\n",
    "]\n",
    "\n",
    "# Split the list into sublists of 10 elements each\n",
    "sublists = [stocks[i:i + 20] for i in range(0, len(stocks), 20)]\n",
    "\n",
    "for sublist in sublists:\n",
    "    print(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_20_1 = [\n",
    "    \"FORTH\", \"STGT\", \"BSRC\", \"THG\", \"AEONTS\", \"TCAP\", \"RCL\", \"AURA\", \"TASCO\", \"RBF\",\n",
    "    \"MOSHI\", \"PLANB\", \"TKN\", \"TOA\", \"CHG\", \"DOHOME\", \"VGI\", \"SJWD\", \"SPALI\", \"SAPPE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_20_2 = [\n",
    "    \"SISB\", \"RATCH\", \"BTG\", \"TLI\", \"GUNKUL\", \"BCPG\", \"HMPRO\", \"MEGA\", \"CENTEL\", \"IRPC\",\n",
    "    \"SNNP\", \"BCH\", \"ICHI\", \"EGCO\", \"AMATA\", \"AAV\", \"JMART\", \"SPRC\", \"AWC\", \"INTUCH\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_20_3 = [\n",
    "    \"SAWAD\", \"COM7\", \"BANPU\", \"CBG\", \"JMT\", \"IVL\", \"KCE\", \"KTC\", \"BYD\", \"TISCO\",\n",
    "    \"PTTGC\", \"SCGP\", \"SCC\", \"TIDLOR\", \"LH\", \"CPN\", \"CPF\", \"SIRI\", \"CRC\", \"MTC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_20_4 = [\n",
    "    \"ERW\", \"MINT\", \"BTS\", \"BEM\", \"KKP\", \"BLA\", \"HANA\", \"GPSC\", \"BH\", \"OSP\",\n",
    "    \"BAM\", \"BDMS\", \"BGRIM\", \"KTB\", \"TRUE\", \"DELTA\", \"WHA\", \"PTTEP\", \"TTB\", \"CPALL\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_20_5 = [\n",
    "    'BCP', 'BBL', 'ADVANC', 'GULF', 'ORI', 'KBANK', 'AOT', 'ITC', 'NEX',\n",
    "    'GLOBAL', 'PTT', 'SCB', 'TOP', \n",
    "    'ศรีตรังแอโกรอินดัสทรี', 'ศรีตรัง', 'ไทยยูเนี่ยน กรุ๊ป จำกัด', 'ไทยยูเนี่ยน','ช.การช่าง','ปตท. น้ำมันและการค้าปลีก', 'น้ำมันและการค้าปลีก','เอพี', 'เอ็มเค', 'เอ็มเค เรสโตรองต์', 'พลังงานบริสุทธิ์']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver = webdriver.Chrome()\n",
    "url = 'https://www.settrade.com/th/news-and-articles/news/main'\n",
    "driver.get(url)\n",
    "perform_initial_setup(driver)\n",
    "\n",
    "all_data1 = []\n",
    "for stock in stock_20_1:\n",
    "    concat_df = scraping(driver, stock)\n",
    "    if concat_df is not None:\n",
    "        all_data1.append(concat_df)\n",
    "driver.quit()\n",
    "\n",
    "if all_data1:\n",
    "    final_df = pd.concat(all_data1, ignore_index=True)\n",
    "    file_name = \"1y_set100_stock_20_1.csv\"\n",
    "    final_df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "    print(f\"All data saved to {file_name}\")\n",
    "else:\n",
    "    print(\"No data scraped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def perform_initial_setup(driver):\n",
    "    try:\n",
    "        # load\n",
    "        time.sleep(random.uniform(4, 6))\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, '.heading-font-family.fs-20px.text-nowrap'))\n",
    "        )\n",
    "\n",
    "        # advanced search\n",
    "        adv_search = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.heading-font-family.fs-20px.text-nowrap')\n",
    "        adv_search.click()\n",
    "        print(\"Advanced search clicked\")\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # period \n",
    "        period = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.col-auto.my-auto')\n",
    "        period.click()\n",
    "        print(\"Period input field clicked\")\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # 3 months option\n",
    "        # month3 = driver.find_element(\n",
    "        #    By.XPATH, '//button[contains(text(), \"3M\")]')\n",
    "        # month3.click()\n",
    "        # print(\"3 months option selected\")\n",
    "        # time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # 1y option\n",
    "        year1 = driver.find_element(\n",
    "            By.XPATH, '//button[contains(text(), \"1Y\")]')\n",
    "        year1.click()\n",
    "        print(\"1y option selected\")\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # confirm button\n",
    "        confirm = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.btn.w-50.w-xl-auto.bg-primary.text-white.fs-20px.ms-1.text-nowrap')\n",
    "        confirm.click()\n",
    "        print(\"Confirm button clicked\")\n",
    "        time.sleep(random.uniform(4, 8))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during initial setup: {e}\")\n",
    "\n",
    "\n",
    "def search_stock(driver, stock_name):\n",
    "    try:\n",
    "        # search keyword\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "        search = driver.find_element(\n",
    "            By.CSS_SELECTOR, '.form-control.shadow-none.border-0.px-0.fs-20px')\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollTop);\")\n",
    "        search.clear()\n",
    "        time.sleep(random.uniform(2, 3))\n",
    "        search.send_keys(stock_name)\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        print(f\"{stock_name} searched\")\n",
    "\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located(\n",
    "                (By.XPATH, '//div[@class=\"d-flex me-2\"]//span'))\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"serach error: {e}\")\n",
    "\n",
    "\n",
    "def scrape_num_search(driver, stock_name):\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "\n",
    "        numofnews = driver.find_element(\n",
    "            By.XPATH, '//div[contains(text(), \"ผลการค้นหา\")]')\n",
    "\n",
    "        # Find the class elements\n",
    "        # class_elements = driver.find_elements(\n",
    "        #     By.XPATH, '//div[@class=\"pb-2 fs-12px text-ref default-font-family fw-500 ps-1\"]')\n",
    "        num_news = numofnews.text.split()[1]\n",
    "        num_news2 = numofnews.text\n",
    "        print(f\"Num news for {stock_name}: {num_news}\")\n",
    "        data = {\n",
    "            'Stock Name': [stock_name],\n",
    "            'Num News': [num_news],\n",
    "            'Text num news': [num_news2]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scraping(driver, stock_name):\n",
    "    try:\n",
    "        # initial setup (advanced search, select period, enter search keyword)\n",
    "        search_stock(driver, stock_name)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        numnews_df = scrape_num_search(driver, stock_name)\n",
    "        return numnews_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url = 'https://www.settrade.com/th/news-and-articles/news/main'\n",
    "driver.get(url)\n",
    "perform_initial_setup(driver)\n",
    "\n",
    "\n",
    "set100stocks_1 = [\n",
    "    \"AAV\", \"ADVANC\", \"AEONTS\", \"AMATA\", \"AOT\", \"AP\", \"AURA\", \"AWC\", \"BAM\", \"BANPU\",\n",
    "    \"BBL\", \"BCH\", \"BCP\", \"BCPG\", \"BDMS\", \"BEM\", \"BGRIM\", \"BH\", \"BLA\", \"BSRC\",\n",
    "    \"BTG\", \"BTS\", \"BYD\", \"CBG\", \"CENTEL\", \"CHG\", \"CK\", \"COM7\", \"CPALL\", \"CPF\",\n",
    "    \"CPN\", \"CRC\", \"DELTA\", \"DOHOME\", \"EA\", \"EGCO\", \"ERW\", \"FORTH\", \"GLOBAL\", \"GPSC\",\n",
    "    \"GULF\", \"GUNKUL\", \"HANA\", \"HMPRO\", \"ICHI\", \"INTUCH\", \"IRPC\", \"ITC\", \"IVL\", \"JMART\"\n",
    "]\n",
    "\n",
    "set100stocks_2 = [\n",
    "    \"JMT\", \"KBANK\", \"KCE\", \"KKP\", \"KTB\", \"KTC\", \"LH\", \"M\", \"MEGA\", \"MINT\",\n",
    "    \"MOSHI\", \"MTC\", \"NEX\", \"OR\", \"ORI\", \"OSP\", \"PLANB\", \"PTT\", \"PTTEP\", \"PTTGC\",\n",
    "    \"RATCH\", \"RBF\", \"RCL\", \"SAPPE\", \"SAWAD\", \"SCB\", \"SCC\", \"SCGP\", \"SIRI\", \"SISB\",\n",
    "    \"SJWD\", \"SNNP\", \"SPALI\", \"SPRC\", \"STA\", \"STGT\", \"TASCO\", \"TCAP\", \"THG\", \"TIDLOR\",\n",
    "    \"TISCO\", \"TKN\", \"TLI\", \"TOA\", \"TOP\", \"TRUE\", \"TTB\", \"TU\", \"VGI\", \"WHA\"\n",
    "]\n",
    "\n",
    "\n",
    "all_data = []\n",
    "for stock in set100stocks_2:\n",
    "    dfnumnews = scraping(driver, stock)\n",
    "    if dfnumnews is not None:\n",
    "        all_data.append(dfnumnews)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    file_name = \"numnew_6m_set100stocks_2.csv\"\n",
    "    final_df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "    print(f\"All data saved to {file_name}\")\n",
    "else:\n",
    "    print(\"No data scraped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
